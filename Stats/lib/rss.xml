<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Web_Vault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>Web_Vault</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 30 Sep 2024 17:25:26 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 30 Sep 2024 17:25:24 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[CH 1 Continued - Categorical Variables in Statistics]]></title><description><![CDATA[ 
 <br><br><br>Continuing from the previous chapter, we will now look at how to represent categorical data in statistics with an example.<br>1000 ball bearings classified as:

<br>conforming (910)
<br>too thick (53)
<br>too thin (37)

<br>Here, the data is classified into three categories, which are mutually exclusive and exhaustive. The data is categorical because it is classified into categories and not measured.<br>Lets make a table of counts for the data.<br><br><br><br><br><br>import micropip
await micropip.install("matplotlib")
import matplotlib.pyplot as plt

# Data
labels = ['Conforming', 'Too Thick', 'Too Thin']
values = [0.910, 0.053, 0.037]

# Create the bar chart
plt.figure(figsize=(6, 4))
plt.bar(labels, values, color=['green', 'red', 'blue'])

# Add title and labels
plt.title('Bar Chart of Measurements')
plt.xlabel('Measurement Type')
plt.ylabel('Proportion')

# Show the values on top of the bars
for i, value in enumerate(values):
    plt.text(i, value + 0.01, f'{value:.3f}', ha='center', va='bottom')

# Display the chart
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.18.44 AM.png" src="lib/media/screenshot-2024-09-12-at-11.18.44-am.png"><br>This will create a bar chart with the proportion of each category, where the height of the bar represents the proportion of the category.<br><br><br>Now, lets look at another example<br>Chromium VS Nickel Simple Dataset  
Chromium: 31, 1, 511, 2, 574, 496, 322, 424, 269, 140, 244, 252, 76, 108, 24, 38, 18, 43, 30, 191
Nickel: 23, 22, 55, 39, 283, 34, 159, 37, 61, 34, 163, 140, 32, 23, 54, 837, 64, 354, 376, 471
<br>We are going to find:<br>
<br>Mean
<br>Median
<br>Standard Deviation
<br>Quartiles
<br>Lets find these with python.<br>import micropip
await micropip.install("numpy")
await micropip.install("scipy")
await micropip.install("matplotlib")

import numpy as np
import matplotlib.pyplot as plt
import scipy as stats

# Data
chromium = [31, 1, 511, 2, 574, 496, 322, 424, 269, 140, 244, 252, 76, 108, 24, 38, 18, 43, 30, 191]
nickel = [23, 22, 55, 39, 283, 34, 159, 37, 61, 34, 163, 140, 32, 23, 54, 837, 64, 354, 376, 471]

# Mean
mean_chromium = np.mean(chromium)
mean_nickel = np.mean(nickel)

# standard deviation
std_chromium = np.std(chromium)
std_nickel = np.std(nickel)

# Median
median_chromium = np.median(chromium)
median_nickel = np.median(nickel)

# Quartiles
first_quartile_chromium = np.percentile(chromium, 25)
second_quartile_chromium = np.percentile(chromium, 50)
third_quartile_chromium = np.percentile(chromium, 75)
fourth_quartile_chromium = np.percentile(chromium, 100)

firsst_quartile_nickel = np.percentile(nickel, 25)
second_quartile_nickel = np.percentile(nickel, 50)
third_quartile_nickel = np.percentile(nickel, 75)
fourth_quartile_nickel = np.percentile(nickel, 100)

# print the results as f string
print(f"Chromium Mean: {mean_chromium}, Median: {median_chromium}, Standard Deviation: {std_chromium}")
print(f"Chromium Quartiles: {first_quartile_chromium}, {second_quartile_chromium}, {third_quartile_chromium}, {fourth_quartile_chromium}")

print(f"Nickel Mean: {mean_nickel}, Median: {median_nickel}, Standard Deviation: {std_nickel}")
print(f"Nickel Quartiles: {firsst_quartile_nickel}, {second_quartile_nickel}, {third_quartile_nickel}, {fourth_quartile_nickel}")

Copy<br>

<br>Chromium Mean: 189.7, Median: 124.0, Standard Deviation: 183.75693184203965
<br>Chromium Quartiles: 30.75, 124.0, 282.25, 574.0
<br>Nickel Mean: 163.05, Median: 58.0, Standard Deviation: 203.7720478868483
<br>Nickel Quartiles: 34.0, 58.0, 193.0, 837.0

<br>This yield the 1-Variable Statistics we would normally get from a Ti-84 Graphing Calculator.<br>And now let's plot it as a box plot:<br># Boxplot
plt.figure(figsize=(6, 4))
plt.boxplot([chromium, nickel], labels=['Chromium', 'Nickel'])
plt.title('Boxplot of Chromium and Nickel')

# Display the chart
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.19.50 AM.png" src="lib/media/screenshot-2024-09-12-at-11.19.50-am.png">]]></description><link>class-notes/ch1-categorical-variable.html</link><guid isPermaLink="false">Class Notes/CH1 - Categorical Variable.md</guid><pubDate>Thu, 12 Sep 2024 15:19:59 GMT</pubDate><enclosure url="lib/media/screenshot-2024-09-12-at-11.18.44-am.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/screenshot-2024-09-12-at-11.18.44-am.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[CH1 - Sampling and Descriptive Statistics]]></title><description><![CDATA[ 
 <br><br>
How do we get data?<br>
How do we describe data?
<br><br><a data-href="Population" href="conceptual-notes/population.html" class="internal-link" target="_self" rel="noopener">Population</a>: the entire collection of objects or outcomes about which information is sought<br><a data-href="Sample" href="conceptual-notes/sample.html" class="internal-link" target="_self" rel="noopener">Sample</a>: a subset of the population containing the objects or outcomes that are actually observed<br><br>We want to know something about the population. We collect a sample that is representative of the population. We take the measurements/outcomes from the sample to infer/say something about the population.<br>It is very important to have a representative sample.<br>Our best tool to get a representative sample is SIMPLE RANDOM SAMPLE(SRS).<br>An SRS of size "n" is a sample chosen by a method in which each collection of n population items is equally likely to occur.<br>This is the number of ways to choose 5 items from a population of 50 items, where the order of selection does not matter.<br><br>Every time you take a number, the results will vary. This is known as Natural Sampling Variability<br><br>Random Question: What is the population of UVM undergrads that are out of state?<br>Population: all UVM undergrads<br>Sample: STAT2430A, where<br>
<br>A: In State
<br>B: Out of State
<br>Calculations:<br>
<br>From the poll, we can infer that 14% of the population is out of state. However, this might not be the case as the sample is not representative.<br><br>Another Random Question: What proportion of all UVM undergrads are in CEMS?<br>Population: All UVM undergrads<br>Sample: STAT2430A, where<br>
<br>A: CEMS
<br>B: Not CEMS
<br>Calculations:<br>Sample:<br>
<br>Full Population:<br>
<br>From the poll, we can infer that 92% of the population is in CEMS. However, this might not be the case as the sample is not representative. Why is this the case?<br>Answer
STAT2430A is a class that is predominantly CEMS. This is not representative of the population.
<br><br>Understanding the difference between Tangible and Conceptual Population<br>When we say "All UVM undergrads", we are talking about a conceptual population. This is not tangible. We cannot measure this population.<br>
When we say "STAT2430A", we are talking about a tangible population. This is measurable.<br>When measurements are taken from a process under identical experimental conditions, we consider the measurements to be an SRS of the population.<br>These measurements come from a population of all possible values that might be observed.<br><br><br>Data collected in a study can be broadly classified into two types: Categorical and Quantitative.<br><br>Categorical data, also known as qualitative or nominal data, is data that can be divided into several categories but has no order or priority. It is often non-numeric and may include data such as colors (red, blue, green), gender (male, female), or types of cuisine (Italian, Chinese, Mexican).<br>Categorical data can further be classified into:<br>
<br>Nominal data: This is data that cannot be ordered or prioritized. Examples include colors or countries.
<br>Ordinal data: This is data that can be ordered or prioritized but the intervals between data points are not known. Examples include ratings or rankings.
<br><br>Quantitative data, also known as numerical data, is data that is measured or counted. It is always numeric. It includes data such as height (in inches or centimeters), time (in seconds or minutes), or age (in years).<br>Quantitative data can further be classified into:<br>
<br>Discrete data: This is numeric data that has a countable number of values. Examples include the number of students in a class or the number of cars in a parking lot.
<br>Continuous data: This is numeric data that has an infinite number of values within a specified range. Examples include the height of a person or the time taken to run a race.
<br>Understanding the type of data you are dealing with is crucial as it determines the type of statistical analysis that can be performed on the data.]]></description><link>class-notes/ch1-sampling-and-descriptive-statistics.html</link><guid isPermaLink="false">Class Notes/CH1 - Sampling and Descriptive Statistics.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[CH1 - Sampling Statistics Continued]]></title><description><![CDATA[ 
 <br>
How do we find the standard deviation of a sample?<br>
Why is it important to know the standard deviation of a sample?
<br><br>The standard deviation of a sample is a measure of the amount of variation or dispersion of a set of values. It is calculated as the square root of the variance, which is the average of the squared differences from the mean.<br>Here's the formula for standard deviation (σ):<br>
<br>Where:<br>
<br>Σ is the sum of...
<br>(xi - μ)^2: the squared difference between each data point (xi) and the mean (μ)
<br>N is the number of data points in the sample
<br>Knowing the standard deviation of a sample is important because it gives us an understanding of how spread out the values in our data are around the mean. A low standard deviation means that the values tend to be close to the mean, while a high standard deviation means that the values are spread out over a wider range.<br>It's a key concept in statistics and is used in many areas, including finance, physics, and machine learning, to name a few.<br><br><br>Let's consider a simple example with a sample of five values: 2, 4, 6, 8, 10.<br>First, we calculate the mean (μ):<br><br>Next, we calculate the squared differences from the mean:<br>
<br>(2 - 6)^2 = 16
<br>(4 - 6)^2 = 4
<br>(6 - 6)^2 = 0
<br>(8 - 6)^2 = 4
<br>(10 - 6)^2 = 16
<br>Then, we calculate the average of these squared differences (this is the variance):<br><br>Finally, we take the square root of the variance to get the standard deviation:<br><br>So, the standard deviation of our sample is approximately 2.83.<br><br>Variance is a statistical measurement that describes the spread of data points in a data sample. It measures how far each number in the set is from the mean (average) and thus from every other number in the set. Variance is often denoted by the symbol σ².<br>Here's the formula for variance:<br><br>Where:<br>
<br>Σ is the sum of...
<br>(xi - μ)^2: the squared difference between each data point (xi) and the mean (μ)
<br>N is the number of data points in the sample
<br>Variance is used to see how individual numbers relate to each other within a data set, rather than using broader mathematical techniques such as arranging data into quartiles. It's a key concept in many areas of statistics, including finance, physics, and machine learning.<br><br>Continuing with our previous example where we had a sample of five values: 2, 4, 6, 8, 10. We calculated the variance as:<br><br>So, the variance of our sample is 8.<br><br>In <a data-href="1 Variable Stats On Calculator" href="conceptual-notes/1-variable-stats-on-calculator.html" class="internal-link" target="_self" rel="noopener">1 Variable Stats On Calculator</a>, I discuss how you can use the 1-Var Stats function on the TI-84 Plus CE to calculate the mean, sum of x, sum of x squared, sample standard deviation, population standard deviation, and more for a single-variable data set. This is a useful tool for quickly obtaining these statistics for your data.<br><br><br>
	import micropip
	await micropip.install("numpy")
	await micropip.install("matplotlib")
	await micropip.install("scipy")

	from scipy.stats import trim_mean
	import numpy as np
	import matplotlib.pyplot as plt

	# Example data list
	lst = [12, 15, 14, 10, 18, 20, 24, 13, 17, 22]

	# Calculate one-var stats
	mean = np.mean(lst)
	sum_x = np.sum(lst)
	sum_x_squared = np.sum(np.square(lst))
	sample_std_dev = np.std(lst, ddof=1)
	population_std_dev = np.std(lst)
	n = len(lst)
	min_x = np.min(lst)
	q1 = np.percentile(lst, 25)
	median = np.median(lst)
	q3 = np.percentile(lst, 75)
	max_x = np.max(lst)

	# Calculate 20% trimmed mean
	trimmed_mean = trim_mean(lst, 0.2)

	# Print the stats
	print(f"Mean (x̄): {mean}")
	print(f"Sum of x (Σx): {sum_x}")
	print(f"Sum of x squared (Σx²): {sum_x_squared}")
	print(f"Sample Standard Deviation (Sx): {sample_std_dev}")
	print(f"Population Standard Deviation (σx): {population_std_dev}")
	print(f"Number of data points (n): {n}")
	print(f"Minimum value (minX): {min_x}")
	print(f"First Quartile (Q1): {q1}")
	print(f"Median (Med): {median}")
	print(f"Third Quartile (Q3): {q3}")
	print(f"Maximum value (maxX): {max_x}")
	print(f"20% Trimmed Mean: {trimmed_mean}")

	# Plotting the results
	plt.figure(figsize=(6, 4))

	# Boxplot
	plt.subplot(1, 2, 1)
	plt.boxplot(lst, vert=False)
	plt.title('Boxplot of Data')
	plt.xlabel('Value')

	# Histogram with mean and trimmed mean
	plt.subplot(1, 2, 2)
	plt.hist(lst, bins=8, edgecolor='black', alpha=0.7)
	plt.axvline(mean, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean:.2f}')
	plt.axvline(trimmed_mean, color='g', linestyle='dashed', linewidth=2, label=f'Trimmed Mean: {trimmed_mean:.2f}')
	plt.title('Histogram of Data')
	plt.xlabel('Value')
	plt.ylabel('Frequency')
	plt.legend()

	plt.tight_layout()
	plt.show()

Copy<br>

<br>Mean (x̄): 16.5
<br>Sum of x (Σx): 165
<br>Sum of x squared (Σx²): 2907
<br>Sample Standard Deviation (Sx): 4.527692569068709
<br>Population Standard Deviation (σx): 4.295346318982906
<br>Number of data points (n): 10
<br>Minimum value (minX): 10
<br>First Quartile (Q1): 13.25
<br>Median (Med): 16.0
<br>Third Quartile (Q3): 19.5
<br>Maximum value (maxX): 24
<br>20% Trimmed Mean: 16.166666666666668

<br><img alt="Screenshot 2024-09-12 at 11.20.20 AM.png" src="lib/media/screenshot-2024-09-12-at-11.20.20-am.png">]]></description><link>class-notes/ch1-sampling-statistics-continued.html</link><guid isPermaLink="false">Class Notes/CH1 - Sampling Statistics Continued.md</guid><pubDate>Thu, 12 Sep 2024 15:21:32 GMT</pubDate><enclosure url="lib/media/screenshot-2024-09-12-at-11.20.20-am.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/screenshot-2024-09-12-at-11.20.20-am.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[CH2.3 - Conditional Probability and Independence]]></title><description><![CDATA[ 
 <br><br>This section covers conditional probability and independence. Conditional probability is the probability of an event given that another event has occurred. Independence is when the occurrence of one event does not affect the probability of another event.<br><br><br>Example

<br>This probability is an Intersection and joint probability, think of multiply, but only if you can answer the following question.<br><br><br>Question
Are the events Independent of each other?
<br>Independent - the outcome of one event does not effect the probability of the other event<br>Here are some examples of independent events:<br>Independent Events

<br>spin a roulette wheel twice
<br>flip a coin twice
<br>rolling two dice
<br>sampling with replacement

<br>Going back to the question, lets look at the possible answers.<br>Option 1: Yes

<br>Option 2: No

<br>Lets tackle a practice problem now.<br><br><br>Problem
Mobile telephones perform handoffs as they move from call to cell. During a call, a telephone either performs zero handoffs (0), one handoff (H1), or more than one handoff (H2). IN addition, each call is either long (L), if it last more then three minutes, or brief (B). We have the following table that represents the data, a sample of 100 calls:

<br>Research Question: Are lengths and handoffs Independent of Each Other?<br>
<br>Marginal Distributions
<br>Length
Probability of a Long Call<br>
Probability of a Brief Call<br>

<br>Handoffs
Probability of Zero Handoffs<br>
Probability of One Handoff<br>
Probability of More than One Handoff<br>

<br>
<br>
<br>
Conditional Distributions

<br>Condition in a level for one categorical variable given the level of another categorical variable.<br>Calculating for the following probabilities:<br>
<br>Probability of Zero Handoffs given a Long Call
<br>P ( * | L)

<br>
<br>Probability of Zero Handoffs given a Brief Call
<br>P ( * | B)

<br>
<br>Probability of a Long Call given Zero Handoffs
<br>P ( * | H0)

<br>
<br>Probability of a Brief Call given Zero Handoffs
<br>P ( * | H1)

<br>
<br>Probability of a Long Call given More than One Handoff
<br>P ( * | H2)

<br>Bringing it all together, let's calculate the following:<br>Final Calculations

<br><br>import micropip
await micropip.install("matplotlib")
await micropip.install("numpy")

import numpy as np
import matplotlib.pyplot as plt

# define the data
# bar Chart
N = 3
ind = np.arange(N)  # the x locations for the groups
width = 0.35       # the width of the bars

p1 = plt.bar(ind, (0.25, 0.67, 0.67), width, color='r')
p2 = plt.bar(ind, (0.25, 0.17, 0.17), width, color='b', bottom=(0.25, 0.67, 0.67))
p3 = plt.bar(ind, (0.5, 0.17, 0.17), width, color='g', bottom=(0.5, 0.17, 0.17))

# add some text for labels, title and axes ticks
plt.ylabel('Probability')
plt.title('Probability by Length and Handoff')
plt.xticks(ind, ('H0', 'H1', 'H2'))
plt.yticks(np.arange(0, 1.1, 0.1))
plt.legend((p1[0], p2[0], p3[0]), ('L', 'B', 'H2'))

# Display the chart
plt.show()
Copy<br><img alt="graph.png" src="lib/media/graph.png"><br>Either:<br>Conditionals are not similar to marginals, there is an association between the two, and thus, not independent.<br>Or:<br>Conditionals are Similar to Marginals, there is no association between the two, and thus, independent.<br><br><br>Lets find a random combination from that table and determine if they are independent.<br>Question
Are L and H0 independent of each other?<br>

<br>Therefore,  is <br><br><br>Example
If you own two cars, what is the probablity that:

<br>neither will need repair?
<br>both will need repair?
<br>at least one car will need repair?

<br><br><br>
<br>Neither will need repair
<br>Correct Answer

<br>
<br>Both will need repair
<br>Correct Answer

<br>
<br>At least one car will need repair
<br>Correct Answer

<br><br>Classic exmaple: Balls in Urn to Demonstrate Tree Diagrams, Conditional Probability, Joint Probability, and Independence.<br>Example
10 balls in an urn, 3 are orange, 5 are red, and 2 are blue.<br>
Draw two balls without replacement.
<br><br>The following tree diagram is a visual representation of the problem.<br><br>From the tree diagram, we can calculate the following:<br>
<br>Probability of Drawing an Orange Ball
<br>Correct Answer

<br>
<br>Probability of Drawing a Red Ball
<br>Correct Answer

<br>
<br>Probability of Drawing a Blue Ball
<br>Correct Answer

]]></description><link>class-notes/ch2-conditional-probability.html</link><guid isPermaLink="false">Class Notes/CH2 - Conditional Probability.md</guid><pubDate>Thu, 12 Sep 2024 15:18:28 GMT</pubDate><enclosure url="lib/media/graph.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/graph.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[CH2.2 - Counting Methods]]></title><description><![CDATA[ 
 <br><br><br>So far, S has been small enough that we could list all the outcomes. However, as S grows, it becomes impractical to list all outcomes. We need to develop methods to count the number of outcomes in a sample space.<br>There are some counting methods that we will use in this chapter:<br>We will also use the following:<br><br><br><br>
<br>There are n ways to make choice 1.
<br>For each of these n ways, there are n ways to make choice 2.
<br>--&gt; Total number of ways to make both choices is n * n = n^2<br><br><br><br>Permutations are arrangements of objects in a specific order, where ordering of "n" objects is important.<br><br>Permutations Example
how many ways can you arrange the letters in the word math?
<br><br><br><br>ORDER DOES NOT MATTER<br>Combinations are selections of objects in which the order of selection is not important.<br>Combinations Example
50 students apply for a scholarship. How many ways can 5 students be selected to receive the scholarship?
<br>We can use the combination formula:<br><br>And plug in the numbers:<br><br><br><br>There are 17 broken lightbulbs in a box of 100 lightbulbs. A random sample of 3 lightbulbs is chosen without replacement.<br>
<br>How many ways can the sample be chosen?
<br><br>
<br>How many samples contain no broken lightbulbs?
<br><br>
<br>What is the probability that a sample contains no broken lightbulbs?
<br>]]></description><link>class-notes/ch2-counting-methods.html</link><guid isPermaLink="false">Class Notes/CH2 - Counting Methods.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[Probability In statistics]]></title><description><![CDATA[ 
 <br><br><br><br>Experiment: a process the results in an outcome that cannot be predicted in advance with certainty.<br>Toss a Coin
S = {H,T}
<br>Sample space: set of all possible outcomes<br>Rolling a die
S = {1,2,3,4,5,6}
<br>Event: subset of the sample space<br>Back to the examples:<br>Let even A be the roll of 1, A={1}<br>
Let event E be an even role, E={2,4,6}<br><br>Up to this point, you have been a "frequentist"<br><br>Plugging in the numbers:<br><br><br>Axioms are the basic building blocks of probability theory.<br>Here are the three basic axioms:<br>P(a) is greater than or equal to 0, but less than or equal to 1 for all events a<br><br>P(S) is equal to 1<br><br>P(A or B) = P(A) + P(B) - P(A and B)<br><br><br>Setting up the environment:<br>import micropip
await micropip.install("matplotlib")
await micropip.install("matplotlib_venn")

import matplotlib.pyplot as plt
from matplotlib_venn import venn2

# Data
labels = ['A', 'B', 'A and B']


plt.figure(figsize=(6, 4))
Copy<br>
<br>Not in A
<br>venn2(subsets=(1, 0, 0), set_labels=labels)
plt.title('Not in A')
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.22.06 AM.png" src="lib/media/screenshot-2024-09-12-at-11.22.06-am.png"><br>
<br>Not in B
<br>venn2(subsets=(0, 1, 0), set_labels=labels)
plt.title('Not in B')
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.22.23 AM.png" src="lib/media/screenshot-2024-09-12-at-11.22.23-am.png"><br>
<br>A and B
<br>venn2(subsets=(0, 0, 1), set_labels=labels)
plt.title('A and B')
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.23.33 AM.png" src="lib/media/screenshot-2024-09-12-at-11.23.33-am.png"><br>
<br>A or B
<br>venn2(subsets=(1, 1, 1), set_labels=labels)
plt.title('A or B')
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.23.59 AM.png" src="lib/media/screenshot-2024-09-12-at-11.23.59-am.png"><br>
<br>DeMorgans Law
<br>DeMorgan's Law states that the complement of the union of two sets is equal to the intersection of the complements of the two sets.<br>venn2(subsets=(1, 1, 0), set_labels=labels)
plt.title("DeMorgan's Law")
plt.show()
Copy<br><img alt="Screenshot 2024-09-12 at 11.24.36 AM.png" src="lib/media/screenshot-2024-09-12-at-11.24.36-am.png"><br>Lets examine the ideas DeMorgans Law presents.<br><br><br>Are events mutually exclusive?
Depends on the problem.
<br>Lets look at a deck of cards:<br>Deck of Cards
S = {52 cards}<br>
A = {red cards}<br>
B = {face cards}<br>
A and B = {red face cards}
<br>Are A and B mutually exclusive?<br>No, because there are red face cards.
The intersection of A and B is not empty.
<br>test−10−8−6−4−20246810−10−8−6−4−20246810tdsdfasdfasdf]]></description><link>class-notes/ch2-probability.html</link><guid isPermaLink="false">Class Notes/CH2 - Probability.md</guid><pubDate>Thu, 26 Sep 2024 17:42:20 GMT</pubDate><enclosure url="lib/media/screenshot-2024-09-12-at-11.22.06-am.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/screenshot-2024-09-12-at-11.22.06-am.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[TI-84 Plus CE: 1-Var Stats Breakdown]]></title><description><![CDATA[ 
 <br><br>The "1-Var Stats" function on the TI-84 Plus CE provides a summary of key statistics for a single variable data set. Here's what each component of the output means:<br><br>
<br>Definition: The mean (average) of the data set.
<br>Calculation: Sum of all data points divided by the number of data points.
<br>Formula: xˉ=∑xnx̄ = \frac{\sum{x}}{n}xˉ=n∑x​
<br><br>
<br>Definition: The sum of all the data points.
<br>Calculation: Adds up every individual data point in the data set.
<br>Formula: Σx=x1+x2+...+xnΣx = x_1 + x_2 + ... + x_nΣx=x1​+x2​+...+xn​
<br><br>
<br>Definition: The sum of the squares of each data point.
<br>Calculation: Squares each data point and then sums them up.
<br>Formula: Σx2=x12+x22+...+xn2Σx² = x_1² + x_2² + ... + x_n²Σx2=x12​+x22​+...+xn2​
<br><br>
<br>Definition: The standard deviation of the data set when it represents a sample.
<br>Calculation: Measures the average distance of the data points from the mean.
<br>Formula: Sx=∑(xi−xˉ)2n−1Sx = \sqrt{\frac{\sum{(x_i - x̄)^2}}{n - 1}}Sx=n−1∑(xi​−xˉ)2​​
<br><br>
<br>Definition: The standard deviation of the data set when it represents an entire population.
<br>Calculation: Measures the spread of the data points relative to the mean.
<br>Formula: σx=∑(xi−xˉ)2nσx = \sqrt{\frac{\sum{(x_i - x̄)^2}}{n}}σx=n∑(xi​−xˉ)2​​
<br><br>
<br>Definition: The number of data points in the data set.
<br>Calculation: Simply the count of the total number of data entries.
<br><br>
<br>Definition: The smallest data point in the set.
<br>Significance: Indicates the lower bound of the data set.
<br><br>
<br>Definition: The median of the lower half of the data set.
<br>Significance: 25% of the data points are below this value.
<br><br>
<br>Definition: The middle value of the data set when ordered from least to greatest.
<br>Significance: Splits the data set into two equal halves.
<br><br>
<br>Definition: The median of the upper half of the data set.
<br>Significance: 75% of the data points are below this value.
<br><br>
<br>Definition: The largest data point in the set.
<br>Significance: Indicates the upper bound of the data set.
]]></description><link>conceptual-notes/1-variable-stats-on-calculator.html</link><guid isPermaLink="false">Conceptual Notes/1 Variable Stats On Calculator.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[Axiom in Statistics]]></title><description><![CDATA[ 
 <br><br>An axiom in statistics is a foundational, self-evident truth or assumption that forms the basis for statistical reasoning. These are accepted without proof and serve as starting points for the development of more complex theories and methods.<br><br>In the context of probability, several key axioms underpin the theory:<br><br>
<br>For any event ( A ), the probability of ( A ) is always non-negative.
<br><br><br>
<br>The probability of the sample space ( S ), which represents all possible outcomes, is 1.
<br><br><br>
<br>For any two mutually exclusive events ( A ) and ( B ), the probability of either ( A ) or ( B ) occurring is the sum of their individual probabilities.
<br><br>
<br>This extends to countable collections of mutually exclusive events.
<br>These axioms are part of the Kolmogorov Axioms (1933), which form the foundation of modern probability theory.<br><br>
<br>Probability Axioms: Used as the foundation for calculating probabilities in real-world scenarios, from rolling dice to analyzing data.
<br>Inference: Statistical inference methods rely on these axioms to derive conclusions from sample data, making them critical to the field.
<br><br>Axioms in statistics provide a framework for consistent and rigorous reasoning. They help to ensure that statistical methods are built on solid logical principles.]]></description><link>conceptual-notes/axiom.html</link><guid isPermaLink="false">Conceptual Notes/Axiom.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[De Morgan's Law in Statistics]]></title><description><![CDATA[ 
 <br><br>De Morgan's Laws are fundamental principles in set theory and probability theory that describe the relationship between the complement of unions and intersections of sets (or events, in the context of probability). They provide a way to simplify complex expressions involving complements, unions, and intersections.<br><br>Given two events ( A ) and ( B ) in a sample space ( S ), De Morgan's Laws state:<br>
<br>Complement of a Union:
<br><br>
<br>The complement of the union of two events is equal to the intersection of their complements.
<br>
<br>Complement of an Intersection:
<br><br>
<br>The complement of the intersection of two events is equal to the union of their complements.
<br><br>
<br>First Law (Union): The complement of "either A or B happens" is "neither A nor B happens." In other words, if you want both events not to occur, you must exclude both individually.
<br>Second Law (Intersection): The complement of "both A and B happen" is "either A doesn't happen or B doesn't happen (or both)." So, if you're looking to avoid both events happening, you just need to avoid one of them.
<br><br><br>Consider a standard deck of cards, where:<br>
<br>Event ( A ): Drawing a heart (13 hearts in the deck).
<br>Event ( B ): Drawing a face card (12 face cards in the deck: J, Q, K of all suits).
<br><br>
<br>( A \cup B ): The event of drawing either a heart or a face card.
<br>( (A \cup B)^c ): The event of drawing neither a heart nor a face card (i.e., drawing a non-heart, non-face card).
<br>   Applying De Morgan’s Law:<br><br>   This means the complement of drawing a heart or a face card is equivalent to drawing a card that is both not a heart and not a face card.<br><br>
<br>( A \cap B ): The event of drawing a card that is both a heart and a face card (3 cards: Jack, Queen, and King of hearts).
<br>( (A \cap B)^c ): The event of not drawing a card that is both a heart and a face card.
<br>   Applying De Morgan’s Law:<br><br>   This means the complement of drawing both a heart and a face card is equivalent to drawing a card that is either not a heart or not a face card (or both).<br><br>De Morgan's Laws are particularly useful for simplifying complex probability problems involving multiple events. For example, in calculating the probability of the complement of a union or intersection, the laws allow us to break down the problem into more manageable pieces.<br><br>Given events ( A ) and ( B ), the probability of their complement can be expressed using De Morgan's Laws:<br>
<br>Complement of a Union:
<br><br>
<br>Complement of an Intersection:
<br><br>These laws help transform complicated probability expressions into simpler, more intuitive forms.<br><br>De Morgan’s Laws are vital in probability theory and statistics, as they provide an elegant way to work with complements, unions, and intersections of events. By using these laws, we can simplify and solve problems involving the complement of combined events.]]></description><link>conceptual-notes/de-morgan&apos;s-law.html</link><guid isPermaLink="false">Conceptual Notes/De Morgan&apos;s Law.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[Event in Statistics]]></title><description><![CDATA[ 
 <br><br>An event in statistics refers to a specific outcome or a set of outcomes from a random experiment or process. Events are the basic objects of study in probability theory, and they can range from simple occurrences, like rolling a specific number on a die, to more complex combinations of outcomes.<br><br><br>
<br>The sample space is the set of all possible outcomes of a random experiment. An event is a subset of the sample space.
<br>Example: When flipping a coin, the sample space is ( S = { \text{Heads}, \text{Tails} } ).
<br><br>
<br>A simple event (or elementary event) is an event that consists of exactly one outcome.
<br>Example: Rolling a 3 on a six-sided die is a simple event: ( A = { 3 } ).
<br><br>
<br>A compound event involves two or more simple events. It is a subset of the sample space that includes multiple possible outcomes.
<br>Example: Rolling an even number on a die is a compound event: ( B = { 2, 4, 6 } ).
<br><br>
<br>Two events are mutually exclusive (or disjoint) if they cannot both occur at the same time. If ( A ) and ( B ) are mutually exclusive, then ( P(A \cap B) = 0 ).
<br>Example: In a coin toss, the events "heads" and "tails" are mutually exclusive.
<br><br>
<br>Two events are independent if the occurrence of one event does not affect the probability of the other.
<br>Example: Rolling a die and flipping a coin are independent events because the outcome of the die roll does not affect the outcome of the coin flip.
<br><br>
<br>The complement of an event ( A ), denoted ( A^c ), is the set of all outcomes in the sample space that are not in ( A ).
<br>Example: If event ( A ) is rolling a 1 or 2 on a die, the complement ( A^c ) is rolling a 3, 4, 5, or 6.
<br><br>
<br>The union of two events ( A ) and ( B ), denoted ( A \cup B ), represents the event that either ( A ) or ( B ) or both occur.
<br>Example: Rolling a 2 or an even number on a die is the union ( A \cup B = { 2, 4, 6 } ).
<br><br>
<br>The intersection of two events ( A ) and ( B ), denoted ( A \cap B ), represents the event that both ( A ) and ( B ) occur.
<br>Example: If ( A ) is rolling an even number and ( B ) is rolling a number greater than 2, the intersection ( A \cap B = { 4, 6 } ).
<br><br>The probability of an event ( A ), denoted ( P(A) ), is the measure of how likely the event is to occur. It is calculated as the ratio of the number of favorable outcomes to the total number of possible outcomes, assuming each outcome is equally likely.<br><br><br>For a six-sided die:<br>
<br>The event ( A ) = rolling a 3: ( P(A) = \frac{1}{6} ).
<br>The event ( B ) = rolling an even number: ( P(B) = \frac{3}{6} = \frac{1}{2} ).
<br><br>Events are central to probability theory and statistics. Understanding how to define, combine, and calculate the probabilities of different types of events is essential for analyzing random experiments and making statistical inferences.]]></description><link>conceptual-notes/event.html</link><guid isPermaLink="false">Conceptual Notes/Event.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[Population in Statistics]]></title><description><![CDATA[ 
 <br><br>
<br>Population refers to the entire set of individuals, items, or data points that you are interested in studying. It represents the complete group from which you want to draw conclusions.
<br><br>
<br>All students in a university
<br>Every product in a warehouse
<br>All residents of a city
<br><br>
<br>Every individual has the same probability of being included in the sample.
<br>Selection is typically done using random number generators or drawing names from a hat.
<br><br>
<br>Sampling Frame: A list or database that includes all members of the population from which the sample is drawn.
<br>Sampling Error: The difference between the results obtained from a sample and the actual parameters of the population. It is inherent in the sampling process and can be minimized with proper sampling techniques.
<br>Stratified Sampling: A method that divides the population into subgroups (strata) and then randomly samples from each subgroup. This ensures representation across key variables.
]]></description><link>conceptual-notes/population.html</link><guid isPermaLink="false">Conceptual Notes/Population.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[Samples in Statistics]]></title><description><![CDATA[ 
 <br><br>
<br>Sample: A sample is a subset of a population selected for measurement, observation, or questioning, to provide statistical information about the population.
<br><br>
<br>To gather data that can be used to make inferences about the entire population.
<br>To save time and resources by studying a smaller group rather than the entire population.
<br><br>
<br>Simple Random Sample (SRS): A sampling method where every member of the population has an equal chance of being selected.
<br><br>
<br>Equal probability of selection for all individuals.
<br>Typically achieved using random number generators or other randomization methods.
<br>Minimizes bias in sample selection, ensuring that every individual is equally likely to be chosen.
<br><br>
<br>Unbiased: Reduces the risk of bias in sample selection.
<br>Representative: In large samples, SRS tends to produce a sample that is representative of the population.
<br><br>
<br>Not Always Practical: Requires a complete list of the population, which may not always be available.
<br>Sampling Error: There's still a possibility that the sample might not represent the population perfectly, especially with small sample sizes.
<br><br>
<br>Representative Sample: A sample that accurately reflects the characteristics of the population from which it is drawn.
<br><br>
<br>Proportional Representation: The sample mirrors the population in key characteristics such as age, gender, income, etc.
<br>Low Bias: Selection methods are designed to reduce bias, ensuring that no subgroup is over- or under-represented.
<br><br>
<br>Generalizability: Conclusions drawn from a representative sample are more likely to apply to the population as a whole.
<br>Accuracy: The more representative the sample, the more accurate the results and inferences will be.
<br><br>
<br>Stratified Sampling: Dividing the population into subgroups (strata) and sampling from each subgroup to ensure all key characteristics are represented.
<br>Quota Sampling: Ensuring that the sample meets certain quotas for different subgroups to reflect the population accurately.
<br><br>
<br>Simple Random Sample (SRS) is a method of selection, while Representative Sample is a quality or characteristic of a sample.
<br>An SRS might not always be representative if the population has significant subgroups, but it is often used as a way to achieve a representative sample.
<br><br>
<br>Sampling Bias: Occurs when some members of the population are more likely to be selected than others, leading to a non-representative sample.
<br>Sampling Frame: The list of individuals from which a sample is actually drawn. The accuracy of this list impacts the representativeness of the sample.
<br>Sampling Error: The difference between the characteristics of the sample and those of the population. Can be minimized with larger and more representative samples.
]]></description><link>conceptual-notes/sample.html</link><guid isPermaLink="false">Conceptual Notes/Sample.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item><item><title><![CDATA[Statistics Notes]]></title><description><![CDATA[ 
 <br><br>A collection of my class notes for STAT2430<br><br><br><br><a data-href="CH1 - Sampling and Descriptive Statistics" href="class-notes/ch1-sampling-and-descriptive-statistics.html" class="internal-link" target="_self" rel="noopener">CH1 - Sampling and Descriptive Statistics</a><br>
<a data-href="CH1 - Sampling Statistics Continued" href="class-notes/ch1-sampling-statistics-continued.html" class="internal-link" target="_self" rel="noopener">CH1 - Sampling Statistics Continued</a><br>
<a data-href="CH1 - Categorical Variable.pdf" href="CH1 - Categorical Variable.pdf" class="internal-link" target="_self" rel="noopener">CH1 - Categorical Variable.pdf</a><br><br><a data-href="CH2 - Probability" href="class-notes/ch2-probability.html" class="internal-link" target="_self" rel="noopener">CH2 - Probability</a><br>
<a data-href="CH2 - Counting Methods" href="class-notes/ch2-counting-methods.html" class="internal-link" target="_self" rel="noopener">CH2 - Counting Methods</a><br>
<a data-href="CH2 - Conditional Probability" href="class-notes/ch2-conditional-probability.html" class="internal-link" target="_self" rel="noopener">CH2 - Conditional Probability</a><br><br><br><br><a data-href="Sample" href="conceptual-notes/sample.html" class="internal-link" target="_self" rel="noopener">Sample</a><br>
<a data-href="Axiom" href="conceptual-notes/axiom.html" class="internal-link" target="_self" rel="noopener">Axiom</a><br>
<a data-href="Event" href="conceptual-notes/event.html" class="internal-link" target="_self" rel="noopener">Event</a><br>
<a data-href="Population" href="conceptual-notes/population.html" class="internal-link" target="_self" rel="noopener">Population</a><br><br><a data-href="1 Variable Stats On Calculator" href="conceptual-notes/1-variable-stats-on-calculator.html" class="internal-link" target="_self" rel="noopener">1 Variable Stats On Calculator</a><br><br><a data-href="De Morgan's Law" href="conceptual-notes/de-morgan's-law.html" class="internal-link" target="_self" rel="noopener">De Morgan's Law</a>]]></description><link>statistics-index.html</link><guid isPermaLink="false">Statistics Index.md</guid><pubDate>Thu, 12 Sep 2024 15:08:00 GMT</pubDate></item></channel></rss>